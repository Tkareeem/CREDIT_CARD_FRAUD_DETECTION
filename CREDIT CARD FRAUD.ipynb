{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 23498,
          "sourceType": "datasetVersion",
          "datasetId": 310
        },
        {
          "sourceId": 37500001,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook0b6d5b17c3",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "organizations_mlg_ulb_creditcardfraud_path = kagglehub.dataset_download('organizations/mlg-ulb/creditcardfraud')\n",
        "kartik2112_fraud_detection_on_paysim_dataset_path = kagglehub.notebook_output_download('kartik2112/fraud-detection-on-paysim-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "E_tEih1PFpej"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "14UDXgenFpel"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, accuracy_score)\n",
        "\n",
        "\n",
        "#Load Dataset\n",
        "\n",
        "\n",
        "\n",
        "DATA_PATH = '../input/creditcardfraud/creditcard.csv'\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Dataset Loaded.\")\n",
        "print(\"Shape of Data:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "print(\"\\n--- Checking for Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Class Distribution (Fraud=1, Legit=0) ---\")\n",
        "print(df['Class'].value_counts())\n",
        "\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class'].values\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\n--- Data Split ---\")\n",
        "print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "modelA = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')  # binary classification => use sigmoid\n",
        "])\n",
        "\n",
        "modelA.compile(optimizer=Adam(learning_rate=0.001),\n",
        "               loss='binary_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "print(\"\\n--- Training Model A (Neural Net) ---\")\n",
        "historyA = modelA.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1,  # 10% of train set used for validation\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(historyA.history['loss'], label='Train Loss')\n",
        "plt.plot(historyA.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model A - Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(historyA.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(historyA.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Model A - Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "pred_probs_A = modelA.predict(X_test)\n",
        "pred_A = (pred_probs_A > 0.5).astype(\"int32\")  # convert probabilities to 0/1\n",
        "\n",
        "accA = accuracy_score(y_test, pred_A)\n",
        "aucA = roc_auc_score(y_test, pred_probs_A)\n",
        "print(f\"\\nModel A (Neural Net) - Accuracy: {accA:.4f}\")\n",
        "print(f\"Model A (Neural Net) - AUC: {aucA:.4f}\")\n",
        "\n",
        "cmA = confusion_matrix(y_test, pred_A)\n",
        "print(\"\\nConfusion Matrix (Model A):\")\n",
        "print(cmA)\n",
        "print(\"\\nClassification Report (Model A):\")\n",
        "print(classification_report(y_test, pred_A, digits=4))\n",
        "\n",
        "modelB = LogisticRegression(max_iter=1000)\n",
        "modelB.fit(X_train, y_train)\n",
        "\n",
        "pred_probs_B = modelB.predict_proba(X_test)[:,1]\n",
        "pred_B = modelB.predict(X_test)\n",
        "\n",
        "accB = accuracy_score(y_test, pred_B)\n",
        "aucB = roc_auc_score(y_test, pred_probs_B)\n",
        "print(f\"\\nModel B (Logistic Reg) - Accuracy: {accB:.4f}\")\n",
        "print(f\"Model B (Logistic Reg) - AUC: {aucB:.4f}\")\n",
        "\n",
        "cmB = confusion_matrix(y_test, pred_B)\n",
        "print(\"\\nConfusion Matrix (Model B):\")\n",
        "print(cmB)\n",
        "print(\"\\nClassification Report (Model B):\")\n",
        "print(classification_report(y_test, pred_B, digits=4))\n",
        "\n",
        "\n",
        "ensemble_prob = (pred_probs_A.flatten() + pred_probs_B) / 2.0\n",
        "ensemble_pred = (ensemble_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "accE = accuracy_score(y_test, ensemble_pred)\n",
        "aucE = roc_auc_score(y_test, ensemble_prob)\n",
        "print(f\"\\nEnsemble Model - Accuracy: {accE:.4f}\")\n",
        "print(f\"Ensemble Model - AUC: {aucE:.4f}\")\n",
        "\n",
        "cmE = confusion_matrix(y_test, ensemble_pred)\n",
        "print(\"\\nConfusion Matrix (Ensemble):\")\n",
        "print(cmE)\n",
        "print(\"\\nClassification Report (Ensemble):\")\n",
        "print(classification_report(y_test, ensemble_pred, digits=4))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cmE, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Ensemble Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Dummy Transaction Demo ---\")\n",
        "training_cols = df.drop('Class', axis=1).columns  # columns used for training\n",
        "\n",
        "# Example values: fill them realistically or test extremes\n",
        "dummy_tx = {\n",
        "    \"Time\": 50000,    # if you didn't drop Time\n",
        "    \"V1\": -1.2,\n",
        "    \"V2\": 2.5,\n",
        "    \"V3\": 1.1,\n",
        "    \"V4\": 0.5,\n",
        "    \"V5\": -0.7,\n",
        "    \"V6\": 2.0,\n",
        "    \"V7\": 0.1,\n",
        "    \"V8\": -0.3,\n",
        "    \"V9\": 0.8,\n",
        "    \"V10\": -1.5,\n",
        "    \"V11\": 2.2,\n",
        "    \"V12\": -0.9,\n",
        "    \"V13\": 0.6,\n",
        "    \"V14\": -1.0,\n",
        "    \"V15\": 0.2,\n",
        "    \"V16\": -0.3,\n",
        "    \"V17\": 0.1,\n",
        "    \"V18\": 0.8,\n",
        "    \"V19\": -0.2,\n",
        "    \"V20\": 1.0,\n",
        "    \"V21\": 0.0,\n",
        "    \"V22\": -2.1,\n",
        "    \"V23\": 1.3,\n",
        "    \"V24\": 0.5,\n",
        "    \"V25\": -0.2,\n",
        "    \"V26\": 0.7,\n",
        "    \"V27\": -1.1,\n",
        "    \"V28\": 0.4,\n",
        "    \"Amount\": 300.0\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "dummy_df = pd.DataFrame([dummy_tx])\n",
        "# Ensure columns are in the same order\n",
        "dummy_df = dummy_df[training_cols]\n",
        "\n",
        "# Scale using the SAME scaler\n",
        "dummy_scaled = scaler.transform(dummy_df)\n",
        "\n",
        "# Model A's probability\n",
        "probA_dummy = modelA.predict(dummy_scaled)[0,0]\n",
        "# Model B's probability\n",
        "probB_dummy = modelB.predict_proba(dummy_scaled)[:,1][0]\n",
        "\n",
        "# Ensemble\n",
        "ensemble_prob_dummy = (probA_dummy + probB_dummy)/2.0\n",
        "\n",
        "print(\"Neural Net Fraud Probability:\", probA_dummy)\n",
        "print(\"Logistic Reg Fraud Probability:\", probB_dummy)\n",
        "print(\"Ensemble Probability:\", ensemble_prob_dummy)\n",
        "\n",
        "if ensemble_prob_dummy > 0.5:\n",
        "    print(\"Ensemble Decision: FRAUDULENT Transaction\\n\")\n",
        "else:\n",
        "    print(\"Ensemble Decision: LEGITIMATE Transaction\\n\")\n",
        "\n",
        "print(\"\\n--- Done! ---\\n\")\n",
        "print(\"Observations:\")\n",
        "print(\"1) We trained two models (NN & Logistic) and compared their AUC & accuracy.\")\n",
        "print(\"2) We formed an ensemble by averaging predicted fraud probabilities.\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T23:13:10.105534Z",
          "iopub.execute_input": "2025-04-10T23:13:10.105917Z"
        },
        "id": "xpQCY1T9Fpem"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                             roc_auc_score, accuracy_score)\n",
        "\n",
        "\n",
        "#Load Dataset\n",
        "DATA_PATH = '../input/creditcardfraud/creditcard.csv'\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Dataset Loaded.\")\n",
        "print(\"Shape of Data:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- Checking for Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Class Distribution (Fraud=1, Legit=0) ---\")\n",
        "print(df['Class'].value_counts())\n",
        "\n",
        "\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class'].values\n",
        "\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "\n",
        "processed_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "print(\"\\n--- SCALED DATA PREVIEW (first 5 rows) ---\")\n",
        "print(processed_df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\n--- Data Split ---\")\n",
        "print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "# Model A: Neural Network\n",
        "modelA = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')  # binary classification => use sigmoid\n",
        "])\n",
        "\n",
        "modelA.compile(optimizer=Adam(learning_rate=0.001),\n",
        "               loss='binary_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "print(\"\\n--- Training Model A (Neural Net) ---\")\n",
        "historyA = modelA.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(historyA.history['loss'], label='Train Loss')\n",
        "plt.plot(historyA.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model A - Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(historyA.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(historyA.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Model A - Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "pred_probs_A = modelA.predict(X_test)\n",
        "pred_A = (pred_probs_A > 0.5).astype(\"int32\")  # convert probabilities to 0/1\n",
        "\n",
        "accA = accuracy_score(y_test, pred_A)\n",
        "aucA = roc_auc_score(y_test, pred_probs_A)\n",
        "print(f\"\\nModel A (Neural Net) - Accuracy: {accA:.4f}\")\n",
        "print(f\"Model A (Neural Net) - AUC: {aucA:.4f}\")\n",
        "\n",
        "cmA = confusion_matrix(y_test, pred_A)\n",
        "print(\"\\nConfusion Matrix (Model A):\")\n",
        "print(cmA)\n",
        "print(\"\\nClassification Report (Model A):\")\n",
        "print(classification_report(y_test, pred_A, digits=4))\n",
        "\n",
        "\n",
        "# Model B: Logistic Regression\n",
        "modelB = LogisticRegression(max_iter=1000)\n",
        "modelB.fit(X_train, y_train)\n",
        "\n",
        "pred_probs_B = modelB.predict_proba(X_test)[:,1]\n",
        "pred_B = modelB.predict(X_test)\n",
        "\n",
        "accB = accuracy_score(y_test, pred_B)\n",
        "aucB = roc_auc_score(y_test, pred_probs_B)\n",
        "print(f\"\\nModel B (Logistic Reg) - Accuracy: {accB:.4f}\")\n",
        "print(f\"Model B (Logistic Reg) - AUC: {aucB:.4f}\")\n",
        "\n",
        "cmB = confusion_matrix(y_test, pred_B)\n",
        "print(\"\\nConfusion Matrix (Model B):\")\n",
        "print(cmB)\n",
        "print(\"\\nClassification Report (Model B):\")\n",
        "print(classification_report(y_test, pred_B, digits=4))\n",
        "\n",
        "\n",
        "# Ensemble\n",
        "ensemble_prob = (pred_probs_A.flatten() + pred_probs_B) / 2.0\n",
        "ensemble_pred = (ensemble_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "accE = accuracy_score(y_test, ensemble_pred)\n",
        "aucE = roc_auc_score(y_test, ensemble_prob)\n",
        "print(f\"\\nEnsemble Model - Accuracy: {accE:.4f}\")\n",
        "print(f\"Ensemble Model - AUC: {aucE:.4f}\")\n",
        "\n",
        "cmE = confusion_matrix(y_test, ensemble_pred)\n",
        "print(\"\\nConfusion Matrix (Ensemble):\")\n",
        "print(cmE)\n",
        "print(\"\\nClassification Report (Ensemble):\")\n",
        "print(classification_report(y_test, ensemble_pred, digits=4))\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cmE, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Ensemble Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Dummy Transaction Demo ---\")\n",
        "training_cols = df.drop('Class', axis=1).columns  # columns used for training\n",
        "\n",
        "dummy_tx = {\n",
        "    \"Time\": 50000,\n",
        "    \"V1\": -1.2,\n",
        "    \"V2\": 2.5,\n",
        "    \"V3\": 1.1,\n",
        "    \"V4\": 0.5,\n",
        "    \"V5\": -0.7,\n",
        "    \"V6\": 2.0,\n",
        "    \"V7\": 0.1,\n",
        "    \"V8\": -0.3,\n",
        "    \"V9\": 0.8,\n",
        "    \"V10\": -1.5,\n",
        "    \"V11\": 2.2,\n",
        "    \"V12\": -0.9,\n",
        "    \"V13\": 0.6,\n",
        "    \"V14\": -1.0,\n",
        "    \"V15\": 0.2,\n",
        "    \"V16\": -0.3,\n",
        "    \"V17\": 0.1,\n",
        "    \"V18\": 0.8,\n",
        "    \"V19\": -0.2,\n",
        "    \"V20\": 1.0,\n",
        "    \"V21\": 0.0,\n",
        "    \"V22\": -2.1,\n",
        "    \"V23\": 1.3,\n",
        "    \"V24\": 0.5,\n",
        "    \"V25\": -0.2,\n",
        "    \"V26\": 0.7,\n",
        "    \"V27\": -1.1,\n",
        "    \"V28\": 0.4,\n",
        "    \"Amount\": 300.0\n",
        "}\n",
        "\n",
        "dummy_df = pd.DataFrame([dummy_tx])\n",
        "dummy_df = dummy_df[training_cols]\n",
        "\n",
        "dummy_scaled = scaler.transform(dummy_df)\n",
        "\n",
        "probA_dummy = modelA.predict(dummy_scaled)[0,0]\n",
        "probB_dummy = modelB.predict_proba(dummy_scaled)[:,1][0]\n",
        "\n",
        "ensemble_prob_dummy = (probA_dummy + probB_dummy)/2.0\n",
        "\n",
        "print(\"Neural Net Fraud Probability:\", probA_dummy)\n",
        "print(\"Logistic Reg Fraud Probability:\", probB_dummy)\n",
        "print(\"Ensemble Probability:\", ensemble_prob_dummy)\n",
        "\n",
        "if ensemble_prob_dummy > 0.5:\n",
        "    print(\"Ensemble Decision: FRAUDULENT Transaction\\n\")\n",
        "else:\n",
        "    print(\"Ensemble Decision: LEGITIMATE Transaction\\n\")\n",
        "\n",
        "print(\"\\n--- Done! ---\\n\")\n",
        "print(\"Observations:\")\n",
        "print(\"1) We trained two models (NN & Logistic) and compared their AUC & accuracy.\")\n",
        "print(\"2) We formed an ensemble by averaging predicted fraud probabilities.\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-10T23:23:57.023229Z",
          "iopub.execute_input": "2025-04-10T23:23:57.023739Z",
          "iopub.status.idle": "2025-04-10T23:37:23.921757Z",
          "shell.execute_reply.started": "2025-04-10T23:23:57.023705Z",
          "shell.execute_reply": "2025-04-10T23:37:23.920752Z"
        },
        "id": "3_hPo-p7Fpen"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}